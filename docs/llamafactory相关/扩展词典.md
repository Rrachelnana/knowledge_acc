##扩展词典

在传参的`.yaml`文件中加入`--new_special_tokens  "Strat"`

在`llama_factory` 中扩充词典可用 `new_special_tokens` ，该字段属于`llama_factory.hparams.model_args`。

如果`model_args.new_special_tokens`不为空，则将其进行split

```
if self.new_special_tokens is not None:  # support multiple special tokens
     self.new_special_tokens = [token.strip() for token in self.new_special_tokens.split(",")]
```



在`llama_factory.model.loader.py`中，`load_tokenizer`会从`model_args`中读取参数。

```python
if model_args.new_special_tokens is not None:
   num_added_tokens = tokenizer.add_special_tokens(
       dict(additional_special_tokens=model_args.new_special_tokens),
       replace_additional_special_tokens=False,
   )
   logger.info("Add {} to special tokens.".format(",".join(model_args.new_special_tokens)))
   if num_added_tokens > 0 and not model_args.resize_vocab:
       model_args.resize_vocab = True
       logger.warning("New tokens have been added, changed `resize_vocab` to True.")

patch_tokenizer(tokenizer)
```

上述代码中`tokenizer.add_special_tokens(special_tokens_dict,replace_additional_special_tokens) ` 方法：

```python
将一个特殊词典（如eos、pad、cls等）添加到编码器中，并将它们链接到类属性。如果特殊词不在词汇表中，则将它们添加到词汇表中（从当前词汇表的最后一个索引开始编号）。

使用add_special_tokens将确保你的特殊词可以以多种方式使用：
使用skip_special_tokens = True解码时可以跳过特殊词。
特殊词由分词器小心处理（它们从不被分割），类似于AddedTokens。
你可以轻松地使用分词器类属性（如tokenizer.cls_token）来引用特殊词。这使得开发与模型无关的训练和微调脚本变得容易。
在可能的情况下，特殊词已经为提供的预训练模型注册（例如[BertTokenizer] cls_token已注册为*'[CLS]'*，而XLM的已注册为'</s> '）。

参数：
special_tokens_dict（字典 str 到 str 或 tokenizers.AddedToken）：
键应在预定义的特殊属性列表中：[bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token, additional_special_tokens]。
只有在词汇表中不存在时，词才会被添加（通过检查分词器是否将unk_token的索引分配给它们来测试）。

replace_additional_special_tokens（bool，可选，默认为True）：
如果为True，现有的额外特殊词列表将被special_tokens_dict中提供的列表替换。否则，self._additional_special_tokens只是被扩展。在前一种情况下，词不会从分词器的完整词汇表中移除——它们只是被标记为非特殊词。记住，这只影响哪些词在解码过程中被跳过，而不是added_tokens_encoder和added_tokens_decoder。这意味着之前的additional_special_tokens仍然是已添加的词，并且不会被模型分割。
```

在向词汇表中添加新词时，需同时调整模型的词嵌入矩阵的大小，使其与分词器相匹配。为此，使用`[~PreTrainedModel.resize_token_embeddings]`方法。



`resize_token_embeddings`在`llama_factory.model.model_utils`的`embedding.py`文件中

```python
if len(tokenizer) > current_embedding_size:
    if getattr(model, "quantization_method", None):
    # 检查模型是否已经过量化。如果模型是量化的，那么它不允许调整嵌入层的大小，因为这可能会破坏量化的效果。
        raise ValueError("Cannot resize embedding layers of a quantized model.")

    if not isinstance(model.get_output_embeddings(), torch.nn.Linear):
    # 检查模型的输出嵌入是否是torch.nn.Linear类型。如果不是，说明模型不支持调整嵌入层大小。
        raise ValueError("Current model does not support resizing embedding layers.")

	# resize_token_embeddings方法来调整嵌入层的大小。
	# len(tokenizer)是新的词汇表大小，pad_to_multiple_of=64意味着新的嵌入层大小会被调整为64的倍数，这有助于某些硬件上的性能优化。
    model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=64)
    # with语句块提供了一个上下文管理器，可能是用来处理某些特定的运行时环境或配置
    with context_maybe_zero3:
    	# 获取调整大小后的嵌入层的新的词汇表大小。
        new_embedding_size = model.get_input_embeddings().weight.size(0)
        num_new_tokens = new_embedding_size - current_embedding_size
        # 对新增加的嵌入向量进行初始化，使用一种带噪声的平均初始化方法。这有助于模型更好地适应新增加的词汇。
        _noisy_mean_initialization(model.get_input_embeddings().weight.data, num_new_tokens)
        # 对输出嵌入层的新增加部分进行相同的初始化处理。
        _noisy_mean_initialization(model.get_output_embeddings().weight.data, num_new_tokens)

    logger.info("Resized token embeddings from {} to {}.".format(current_embedding_size, new_embedding_size))
```

`model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=64)` 

如果模型类有`tie_weights()`方法，之后会处理权重嵌入的绑定。

嵌入矩阵中的新词数量。增加大小将在末尾添加新初始化的向量。减少大小将从末尾移除向量。如果未提供或为`None`，则只是返回指向模型输入词`torch.nn.Embedding`模块的指针，而不做任何操作。

`pad_to_multiple_of`如果设置，将嵌入矩阵的大小调整为提供的值的倍数。如果`new_num_tokens`设置为`None`，则只是将嵌入的大小调整为`pad_to_multiple_of`的倍数。



案例介绍：



​		现用的`model`为`Qwen2-7B-Instruct`，`lora`的参数量为：`trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643` 使用了两张`v100`的`40G`卡，分别占用了`32G`、`34G`左右。此处更新的`linear modules`有`Found linear modules: v_proj,up_proj,q_proj,k_proj,o_proj,down_proj,gate_proj`



​		增加了三个`token`后，`lora`的参数量为：`trainable params: 1,110,179,840 || all params: 8,725,796,352 || trainable%: 12.7230`，直接`out of memory`。

​		在增加了token后，有warning：`llamafactory.model.adapter - Vocab has been resized, add embed_tokens,lm_head to trainable params`，其在`llamafactory.model.adapter`文件中。需要增加调整参数的`linear modules`：` gate_proj,q_proj,v_proj,down_proj,k_proj,up_proj,o_proj` ，其在`llamafactory.model.model_utils.misc` 文件中 。

```
if model_args.resize_vocab and finetuning_args.additional_target is None:
	input_embeddings = model.get_input_embeddings()
	output_embeddings = model.get_output_embeddings()
	module_names = set()
	for name, module in model.named_modules():
		if module in [input_embeddings, output_embeddings]:
			module_names.add(name.split(".")[-1])

	finetuning_args.additional_target = module_names
	logger.warning("Vocab has been resized, add {} to trainable params.".format(",".join(module_names)))
```



